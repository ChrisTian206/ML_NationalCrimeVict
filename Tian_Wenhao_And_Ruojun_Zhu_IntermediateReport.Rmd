## 1. Load Data and packages
```{r}
data = read.csv("../cs699/project_dataset.csv")
set.seed(123)
library(FSelector)
library(Boruta)
library(RWeka)
library(caret)
library(rsample)
library(clue)
library(modeest)
library(DMwR)

## Use more cores to accelerate training
library(doParallel)

# Set up a parallel backend using multiple cores
cl <- makeCluster(detectCores())

# Register the parallel backend
registerDoParallel(cl)

#http://cran.nexr.com/web/packages/DMwR/DMwR.pdata
#install dependencies first: ROCR, zoo, xts, quantmod
#then, install package tar.gz. file downloaded into your local machine
#download link: https://cran.r-project.org/src/contrib/Archive/DMwR/
#Download 'DMwR_0.4.1.tar.gz'

#Before
#  0    1 
#3817 1130 

#After
#   0    1  
# 4520 3390 
```

##2. Data Cleanring
```{r}
sum(is.na(data))
```
No missing value found

##3 nearZeroVar
```{r}
##if use nearZ, we left 124
##if use boruta, we have 88
##if use nearZ+boruta, we have 60

nonZ = nearZeroVar(data, names = TRUE)
data = data[, !(names(data) %in% nonZ)]

```
These attributes with zero or near zero variance offers no contribution in improving our model. Therefore, they will be removed prior the next step.

##4 collinearity
```{r}
corr = cor(data[c(1:203)]) 
# Found many missing values. For example, the corr of v2077 and every other attribute is NA. Because every data entry of v2077 is 999 while others are 1,2, or 3. 
# Therefore, I let 
#corr[is.na(corr)] = 0
# Since attributes in our dataset are categorical but represented as numerical values.
# Using correlation does not make sense.
highCorr = findCorrelation(corr, cutoff = 0.7, names = TRUE)
length(highCorr)
highCorr

##feature selection
out = sapply(data, class)
```

## 5. Boruta
```{r}
set.seed(123)
boruta<- Boruta(data$o_bullied~.,data=data)
borutaImp = getSelectedAttributes(boruta, withTentative = F)
borutaImp
boruta

# Boruta returns 64 confirmed important attributes and 
# 22 tentative attributes. We decided to go with Boruta.
data = data[borutaImp]
```

##6. information gain
```{r}
df <- as.data.frame(unclass(data), stringsAsFactors = TRUE)
df$o_bullied <- factor(df$o_bullied)
infogain <- InfoGainAttributeEval(o_bullied ~ . , data = df)
sorted.features <- sort(infogain, decreasing = TRUE)
sorted.features[1:10]
```

##7 PCA
```{r}
data_pca = data
data_pca$o_bullied = factor(data_pca$o_bullied)
sapply(data_pca, class)

set.seed(123)
split = initial_split(data_pca, prop = 0.66, strata = o_bullied)
training = training(split)
test = testing(split)

model = J48(o_bullied ~., data = training)
predicted = data.frame(predict(model,test[-204]))
colnames(predicted)[1] = "predicted"
head(predicted)
cm = table(predicted$predicted, test$o_bullied)
cm
round((sum(diag(cm))/sum(cm))*100,3)
pc = prcomp(training[,-204], center = TRUE)
summary(pc)
```
## 8 cfs
```{r}
subset <- cfs(o_bullied ~., data)
cfs <- as.simple.formula(subset, "o_bullied")
cfs
```

##9 Data Exploration
```{r}
table(sapply(data,class)) # Find that all attribute types are "integer"

nrow(data)
table(data$o_bullied)

library(FSelector)
# CFS:
subset <- cfs(o_bullied ~., data)
cfs <- as.simple.formula(subset, "o_bullied")
cfs
# For data exploration - ploting the pair-wise correlations 
library(GGally)
attributes <- all.vars(cfs)
df_plot <- data[attributes]
ggpairs(df_plot, aes(o_bullied, alpha = 0.5),            
        upper = list(continuous = wrap("cor", size = 3.0)))

# Find the correlation of all attributes in the df_plot
corr_matrix <- cor(df_plot)
corr_matrix
# Plot:
library(ggcorrplot)
ggcorrplot(corr_matrix, method ="square")
```

## 10 Split Data
```{r}
colnames(data)[which(names(data) == "o_bullied")] <- "class"
data$class <- ifelse(data$class ==  0, "N", "Y")
data$class <- as.factor(data$class)
# Assuming your dataframe is named df
data[] <- lapply(data, as.factor)


set.seed(123)
split <- initial_split(data, prop = 0.8, strata = class)
train <- training(split)
test <- testing(split)
```

##11 Imbalance in train data
```{r}

# approach 1, smote
set.seed(123)
summary(train$class) # N 3053 Y 904
train = DMwR::SMOTE(class~., train, k=5, perc.over = 80)
summary(train$class) # N 2892 Y 2712
sum(is.na(train)) # 0

#Approach 2, undersampling
yClass = which(train$class == 'Y')
nClass = which(train$class == 'N')

nsample = length(yClass)
pick_y = sample(yClass, nsample)
pick_n = sample(nClass, nsample)
train = train[c(pick_n,pick_y),]
```


## 11 Gradient Boost Machine
```{r}

ctrl_gbm <- trainControl(method = "CV",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

gbmGrid <- expand.grid(interaction.depth = c(15),
                       n.trees =c(800),
                       shrinkage = c(0.08),
                       n.minobsinnode = c(5))

set.seed(123)

gbmFit <- caret::train(x = train[, -125], 
                y = train$class,
                method = "gbm",
                tuneGrid = gbmGrid,
                metric = "ROC",
                verbose = F,
                trControl = ctrl_gbm,
                )

gbmFit
pred <- predict(gbmFit, test)
cm <- caret::confusionMatrix(pred, test$class)
cm

plot(gbmFit)

 #N = y+200;

#TPR: 80, 65


# N =Y+300
# n.trees = 500, interaction.depth = 7, shrinkage = 0.01
# and n.minobsinnode = 6.
          Reference
Prediction   N   Y
         N 618  89
         Y 146 137

#n.trees = 500 from (250, 500, 750), interaction.depth = 5 from (5,10), shrinkage = 0.01 from (0.01, 0.05, 0.1)
#and n.minobsinnode = 4 from (2,4,6)
          Reference
Prediction   N   Y
         N 617  92
         Y 147 134
         
#Using SMOTE, N 2892 Y 2712, k =5
#n.trees = 750, interaction.depth = 10, shrinkage = 0.05 and n.minobsinnode = 2.
        Reference
Prediction   N   Y
         N 677 130
         Y  87  96
         
#Using SMOTE, N 2892 Y 2712, k = 50
#n.trees = 750, interaction.depth = 5, shrinkage = 0.01 and n.minobsinnode = 4.
         Reference
Prediction   N   Y
         N 687 134
         Y  77  92
         
#Using SMOTE, N 2892 Y 2712, k = 50
#n.trees = 250, interaction.depth = 10, shrinkage = 0.05 and n.minobsinnode = 4.
         
         Reference
Prediction   N   Y
         N 693 139
         Y  71  87
         
         
#N = Y + 200
# n.trees = 150, interaction.depth = 10, shrinkage = 0.02 and n.minobsinnode = 2.
         Reference
Prediction   N   Y
         N 626  96
         Y 138 130
#SMOTE
# n.trees = 150, interaction.depth = 10, shrinkage = 0.02 and n.minobsinnode = 2.
# TPR 84, 61
         Reference
Prediction   N   Y
         N 643  88
         Y 121 138
         
#n.trees = 250, interaction.depth = 15, shrinkage = 0.02 and n.minobsinnode = 2.
#TPR 87, 66
        Reference
Prediction   N   Y
         N 672  75
         Y  92 151
         
#non var, SMOTE
#n.trees = 250, interaction.depth = 15, shrinkage = 0.02 and n.minobsinnode = 2.

         Reference
Prediction   N   Y
         N 696 138
         Y  68  88
         
#non var, N=Y+300
#n.trees = 250, interaction.depth = 10, shrinkage = 0.02 and n.minobsinnode = 1.
          Reference
Prediction   N   Y
         N 619  89
         Y 145 137
        
#SMOTE non var
#lowered k from 50 to 5
#n.trees = 250, interaction.depth = 15, shrinkage = 0.02 and n.minobsinnode = 1.
          Reference
Prediction   N   Y
         N 688 129
         Y  76  97
         
#non var, N=Y
#n.trees = 250, interaction.depth = 10, shrinkage = 0.02 and n.minobsinnode = 2.
         Reference
Prediction   N   Y
         N 535  62
         Y 229 164
#were n.trees = 350, interaction.depth = 15, shrinkage = 0.01 and n.minobsinnode = 1.
         Reference
Prediction   N   Y
         N 522  64
         Y 242 162
#n.trees = 250, interaction.depth = 5, shrinkage = 0.05 and n.minobsinnode = 3.
         Reference
Prediction   N   Y
         N 548  66
         Y 216 160
#n.trees = 150, interaction.depth = 5, shrinkage = 0.05 and n.minobsinnode = 3.
         Reference
Prediction   N   Y
         N 529  65
         Y 235 161
#n.trees = 400, interaction.depth = 5, shrinkage = 0.05 and n.minobsinnode = 3.
         Reference
Prediction   N   Y
         N 538  65
         Y 226 161
#n.trees = 300, interaction.depth = 5, shrinkage = 0.05 and n.minobsinnode = 1.
         Reference
Prediction   N   Y
         N 533  60
         Y 231 166
         
#n.trees = 250, interaction.depth = 3, shrinkage = 0.05 and n.minobsinnode = 2.
         Reference
Prediction   N   Y
         N 540  67
         Y 224 159
         
#n.trees = 250, interaction.depth = 5, shrinkage = 0.05 and n.minobsinnode = 1. 
         Reference
Prediction   N   Y
         N 542  71
         Y 222 155
         
#n.trees = 750, interaction.depth = 10, shrinkage = 0.01 and n.minobsinnode = 3.
         Reference
Prediction   N   Y
         N 544  64
         Y 220 162
         
#used all 204 attr, n.trees = 500, interaction.depth = 5, shrinkage = 0.01 and n.minobsinnode = 2.
         Reference
Prediction   N   Y
         N 550  73
         Y 214 153
         
         
#non-var, SMOTE #N 1446 Y 1627. perc.over = 80
         #params lost.....
         Reference
Prediction   N   Y
         N 600  78
         Y 164 148
         
# n.trees = 100, interaction.depth = 5, shrinkage = 0.35 and n.minobsinnode = 6.
        Reference
Prediction   N   Y
         N 606  86
         Y 158 140
                    
# n.trees = 250, interaction.depth = 5, shrinkage = 0.2 and n.minobsinnode = 6.
        Reference
Prediction   N   Y
         N 594  76
         Y 170 150
```

```{r}

# Class 0
tp = cm$table[1,1]
fp = cm$table[1,2]
tn = cm$table[2,2]
fn = cm$table[2,1]

# Class 1
tp = cm$table[2,2]
fp = cm$table[2,1]
tn = cm$table[1,1]
fn = cm$table[1,2]

calculate_measures <- function(tp, fp, tn, fn){
  tpr = tp / (tp + fn)
  fpr = fp / (fp + tn)
  tnr = tn / (fp + tn)
  fnr = fn / (fn + tp)
  precision = tp / (tp + fp)
  recall = tpr
  f_measure <- (2 * precision * recall) / (precision + recall)
  mcc <- (tp*tn - fp*fn)/(sqrt(tp+fp)*sqrt(tp+fn)*sqrt(tn+fp)*sqrt(tn+fn))
  total = (tp + fn + fp + tn)
  p_o = (tp + tn) / total
  p_e1 = ((tp + fn) / total) * ((tp + fp) / total)
  p_e2 = ((fp + tn) / total) * ((fn + tn) / total)
  p_e = p_e1 + p_e2
  k = (p_o - p_e) / (1 - p_e)
  
  measures <- c('TPR', 'FPR', 'TNR', 'FNR', 'Precision', 'Recall', 'F-measure', 'MCC', 'Kappa')
  values <- c(tpr, fpr, tnr, fnr, precision, recall, f_measure, mcc, k)
  measure.df <- data.frame(measures, values)
  return (measure.df)
}

performance_measures = calculate_measures(tp, fp, tn, fn)
performance_measures
```
##12 SVM
```{r}

set.seed(31)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5, summaryFunction = defaultSummary)
svmGrid <-  expand.grid(sigma = seq(0.1, 0.2, by = 0.05), C = seq(1.0, 2.0, by = 0.2))

model <- caret::train(class ~ ., data = train, method = "svmRadial", preProc = c("center", "scale"),trControl = train_control, tuneGrid = svmGrid)
model
plot(model)

pred <- predict(model, test)
cm <- caret::confusionMatrix(pred,test$class)
cm

#Using SMOTE, N 2892 Y 2712
# sig = 0.2 , C = 2
          Reference
Prediction   N   Y
         N 576 104
         Y 188 122
         
#using 125 attr, and SMOTE 
#sigma = 0.2, C = 1
      Reference
Prediction   N   Y
         N 376   0
         Y 388 226
```


##13 XGBTree
```{r}
ctrl_xgb <- trainControl(method = "CV",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

xgbGrid <- expand.grid(nrounds = c(500), 
                       max_depth = c(7),
                       min_child_weight = c(0.1),
                       eta = c(0.25),
                       subsample = 1,
                       colsample_bytree = 1,
                       gamma = c(0)
                       )

set.seed(31)
xgbFit <- caret::train(x = train[, -125], 
                y = train$class,
                method = "xgbTree",
                tuneGrid = xgbGrid,
                metric = "ROC",
                verbose = FALSE,
                trControl = ctrl_xgb)
xgbFit
plot(xgbFit)

pred <- predict(xgbFit, test)
cm <- caret::confusionMatrix(pred, test$class)
cm

       
#N 1446 Y 1627. nrounds = 500, max_depth = 7, eta = 0.25, gamma = 0, colsample_bytree = 1, min_child_weight = 0.1 and subsample = 1.
#train = DMwR::SMOTE(class~., train, k=5, perc.over = 80)

         Reference
Prediction   N   Y
         N 600  78
         Y 164 148
```

## RF
```{r}
ctrl <- trainControl(method = "CV",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

mtryValues <- c(2,3,4,6)

set.seed(31)
rfFit <- caret::train(x = train[, -125], 
               y = train$class,
               method = "rf",
               ntree = 100,
               tuneGrid = data.frame(mtry = mtryValues),
               importance = TRUE,
               metric = "ROC",
               trControl = ctrl)
rfFit
plot(rfFit)

pred <- predict(rfFit, test)
cm <- caret::confusionMatrix(pred, test$class)
cm
```

```{r}
ctrl = trainControl(method = 'cv', number = 10)
mlp_grid = expand.grid(layer1 = c(1,3),
                       layer2 = c(1,3),
                       layer3 = c(1,3),
                       decay = seq(0.01, 0.05, 0.1))
grid <- expand.grid(layer1 = 1:5, layer2 = 1:5 ,layer3 = 1:5,
                    decay = c(0, .001))

mlp_fit = caret::train(x = train[, -204], 
                       y = train$class, 
                       method = "mlpWeightDecayML", 
                       preProc =  c('center', 'scale'),
                       trControl = ctrl,
                       tuneGrid = grid)

mlp_fit
plot(mlp_fit)

pr <- predict(mlp_fit, newdata = test)
pr
test$class
cm <- caret::confusionMatrix(pr, test$class, positive = 'N')
cm

#N=Y 2712
#were layer1 = 2, layer2 = 2, layer3 = 3 and decay = 0.
Reference
Prediction   N   Y
         N 536  91
         Y 228 135
#layer1 = 5, layer2 = 4, layer3 = 2 and decay = 0.
         Reference
Prediction   N   Y
         N 502  87
         Y 262 139
         
#layer1 = 8, layer2 = 5, layer3 = 5 and decay = 0.
         Reference
Prediction   N   Y
         N 502  87
         Y 262 139
```
## nnet
```{r}
ctrl <- trainControl(method = "CV", number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
## size: number of units in the hidden layer
## decay: parameter for weight decay; also referrred to as L2 regularization
nnetGrid <- expand.grid(size = 4:10, decay = c(0, .1, 1, 2))

set.seed(31)
nnetFit <- train(x = train[, -125], 
                 y = train$class,
                 method = "nnet",
                 metric = "ROC",
                 preProc = c("center", "scale"),
                 tuneGrid = nnetGrid,
                 trace = FALSE,
                 maxit = 100,
                 MaxNWts = 1000,
                 trControl = ctrl)
nnetFit
nnetFit$bestTune
plot(nnetFit)

test_pred <- predict(nnetFit, newdata = test)
test_pred

confusionMatrix(test_pred, test$class)

#N=Y=2712
#size = 5 and decay = 1. maxit = 100, maxnwts = 1000
          Reference
Prediction   N   Y
         N 512  86
         Y 252 140
         
#size = 7and decay = 1.maxit = 100, maxnwts = 1000
         Reference
Prediction   N   Y
         N 520  83
         Y 244 143
         
#size = 7 and decay = 2. maxit = 300, maxnwts = 1000
         Reference
Prediction   N   Y
         N 525  90
         Y 239 136
```



## Close cores usages
```{r}
stopCluster(cl)
```

```{r}
vs0010, 
```




